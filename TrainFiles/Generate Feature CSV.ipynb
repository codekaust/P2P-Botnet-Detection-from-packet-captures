{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import dpkt\n",
    "import subprocess\n",
    "import socket\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import concurrent.futures\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benign Data\n",
    "BEN_1 = glob.glob('../../datasets/d/Benign/p2pbox1/*')\n",
    "BEN_2 = glob.glob('../../datasets/d/Benign/p2pbox2/*')\n",
    "BEN_T = glob.glob('../../datasets/d/Benign/torrent/*')\n",
    "\n",
    "BEN_IP = {'p2p_1':'192.168.1.2', 'p2p_2':'192.168.2.2', 'tor':'172.27.28.106'}\n",
    "\n",
    "# print(BEN_1, BEN_2, BEN_T, BEN_IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botnet Data\n",
    "BOT_S = glob.glob('../../datasets/d/Botnet/storm/*')\n",
    "BOT_Z = glob.glob('../../datasets/d/Botnet/zeus/*')\n",
    "BOT_V = glob.glob('../../datasets/d/Botnet/vinchuca/*')\n",
    "\n",
    "STORM_IP = '''\n",
    "66.154.80.101\n",
    "66.154.80.105\n",
    "66.154.80.111\n",
    "66.154.80.125\n",
    "66.154.83.107\n",
    "66.154.83.113\n",
    "66.154.83.138\n",
    "66.154.83.80\n",
    "66.154.87.39\n",
    "66.154.87.41\n",
    "66.154.87.57\n",
    "66.154.87.58\n",
    "66.154.87.61\n",
    "'''.split()\n",
    "ZEUS_IP = ['10.0.2.15']\n",
    "VIN_IP = ['172.27.22.206']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 3600 # 1 hr in sec\n",
    "TSHARK_FIELDS = \" -e _ws.col.Protocol -e tcp.srcport -e tcp.dstport -e udp.srcport -e udp.dstport -e _ws.col.Time -e _ws.col.Source -e _ws.col.Destination -e _ws.col.Length -e tcp.len -e udp.length -e tcp\"\n",
    "\n",
    "fields = '''\n",
    "protocol\n",
    "srcport\n",
    "dstport\n",
    "\n",
    "time\n",
    "\n",
    "source\n",
    "destination\n",
    "\n",
    "totallen\n",
    "datalen\n",
    "\n",
    "istcp\n",
    "\n",
    "src_dst\n",
    "sip_dip\n",
    "'''.split()\n",
    "\n",
    "fields_dict = dict()\n",
    "\n",
    "i = 0\n",
    "for field in fields: \n",
    "    fields_dict[field]=i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process file function\n",
    "def parse_cap_tr(file_name):\n",
    "    command= \"tshark -r \"+file_name+\" -Y \"+\"'ip.version==4&&(tcp||udp)&&(!dns)'\"+ \\\n",
    "        \" -T fields \" + TSHARK_FIELDS + \" -E separator='|' 2>/dev/null\"\n",
    "            \n",
    "    p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    outs =[]\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while True:\n",
    "        print(f'\\rPacket: {i}',end = '')\n",
    "        i+=1\n",
    "        \n",
    "        line = p.stdout.readline()\n",
    "        \n",
    "        if not line:\n",
    "            break\n",
    "        \n",
    "        line = line.decode('utf-8').strip()\n",
    "        \n",
    "        splits = line.split('|')\n",
    "        \n",
    "        l = []\n",
    "\n",
    "        try:\n",
    "            # append protocol\n",
    "            l.append(splits[0].strip())\n",
    "\n",
    "            # if TCP, append ports\n",
    "            if splits[11].strip()!='':\n",
    "                l.append(int(splits[1].strip()))\n",
    "                l.append(int(splits[2].strip()))\n",
    "            else:\n",
    "                l.append(int(splits[3].strip()))\n",
    "                l.append(int(splits[4].strip()))\n",
    "\n",
    "            # append time\n",
    "            l.append(float(splits[5].strip()))\n",
    "\n",
    "            # append src, dst ip\n",
    "            l.extend(splits[6:8])\n",
    "\n",
    "            # append totallen\n",
    "            l.append(int(splits[8].strip()))\n",
    "\n",
    "            # if TCP, append datalen and istcp\n",
    "            if splits[11].strip()!='':\n",
    "                try:\n",
    "                    l.append(int(splits[9].strip()))\n",
    "                except:\n",
    "                    l.append(0)\n",
    "                l.append(1)\n",
    "            else:\n",
    "                try:\n",
    "                    l.append(int(splits[10].strip()))\n",
    "                except:\n",
    "                    l.append(0)\n",
    "                l.append(0)\n",
    "\n",
    "            # append src+dst for grouping, and sip+dip\n",
    "            l.append(min(splits[6],splits[7])+'__'+max(splits[6],splits[7]))\n",
    "            \n",
    "            l.append(str(min(l[fields_dict['srcport']], l[fields_dict['dstport']]))+'__'+str(max(l[fields_dict['srcport']], l[fields_dict['dstport']])))\n",
    "\n",
    "            index = int(int(l[fields_dict['time']])/TIME_WINDOW)\n",
    "\n",
    "            if index >= len(outs):\n",
    "                out = [l]\n",
    "                outs.append(out)\n",
    "            else:\n",
    "                outs[index].append(l)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"ER1\", e, line, splits)\n",
    "\n",
    "    print('\\r')\n",
    "    \n",
    "    try:\n",
    "        dfs = []\n",
    "        for out in outs:\n",
    "            df = pd.DataFrame(out, columns = fields)\n",
    "            dfs.append(df)\n",
    "        return dfs\n",
    "    except Exception as e:\n",
    "        print(\"ER2\", e, outs, fields)\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function (returns list of list of 23 (18 used in classification) faetures)\n",
    "def extr_feats_tr(df, file_c):\n",
    "    iplist = []\n",
    "    if(file_c == 'z'):\n",
    "        iplist = ZEUS_IP\n",
    "    elif(file_c == 's'):\n",
    "        iplist = STORM_IP\n",
    "    elif(file_c == 'v'):\n",
    "        iplist = VIN_IP\n",
    "  \n",
    "    groups = df.groupby(['src_dst','protocol','sip_dip'])\n",
    "    \n",
    "    features = []\n",
    "\n",
    "    for key in groups.groups:\n",
    "        \n",
    "        f = [0 for x in range (0,24)]\n",
    "        \n",
    "        f[0] = 0.0\n",
    "        \n",
    "        f[6] = 9223372036854775807\n",
    "        f[14] = f[16] = 0.0\n",
    "        f[9] = f[15] = f[17] = 3601.0\n",
    "        \n",
    "        time_last = -1\n",
    "        time_last_f = -1\n",
    "        time_last_b = -1\n",
    "        \n",
    "        f_count = 0\n",
    "        b_count = 0\n",
    "        \n",
    "        tmp_src = key[0].split('__')[0]\n",
    "        tmp_dst = key[0].split('__')[1]\n",
    "        \n",
    "        # Last feature is 0,1,2\n",
    "        # label 0 denotes -> src_ip = botnet\n",
    "        # label 1 denotes -> dst_ip = botnet\n",
    "        # label 2 denotes -> both ips = botnet\n",
    "        f[23] = -1\n",
    "        _a = tmp_src in iplist\n",
    "        _b = tmp_dst in iplist\n",
    "\n",
    "        if _a and (not _b):\n",
    "            f[23]=0\n",
    "        elif (not _a) and _b:\n",
    "            f[23]=1\n",
    "        elif (_a and _b):\n",
    "            f[23]=2\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####### FLOW_DETAILS_NOT_USED_IN_CLASSIFICATION (just used in output of detection model)#################\n",
    "        f[18] = tmp_src\n",
    "        f[19] = tmp_dst\n",
    "        f[20] = key[2].split('__')[0]\n",
    "        f[21] = key[2].split('__')[1]\n",
    "        f[22] = key[1]\n",
    "        ##########################################################################################################\n",
    "        \n",
    "        \n",
    "        i = 1        \n",
    "        \n",
    "        time_last = -1\n",
    "\n",
    "        for _, row in groups.get_group(key).iterrows():\n",
    "            \n",
    "            if(time_last == -1):\n",
    "                time_last = row['time']\n",
    "            \n",
    "            t_diff = row['time']-time_last\n",
    "            time_last = row['time']\n",
    "            \n",
    "            \n",
    "            # f1 (left, divide by nos)\n",
    "            f[0] += t_diff\n",
    "            \n",
    "            # f2, f3, f4, f5\n",
    "            if row['source'] == tmp_src:                \n",
    "                f[1]+=1\n",
    "                \n",
    "                f[3]+=row['datalen']\n",
    "            else:\n",
    "                f[2]+=1\n",
    "                \n",
    "                f[4]+=row['datalen']\n",
    "                \n",
    "                \n",
    "            # f6\n",
    "            f[5]+=row['totallen']\n",
    "            \n",
    "            # f7, f8\n",
    "            f[6] = min(f[6], row['totallen'])\n",
    "            f[7] = max(f[7], row['totallen'])\n",
    "            \n",
    "            # f9, f10\n",
    "            f[8] = max(f[8], t_diff)\n",
    "            \n",
    "            if(t_diff!=0.0):\n",
    "                f[9] = min(f[9], t_diff)\n",
    "            \n",
    "            # f11\n",
    "            f[10] += t_diff\n",
    "            \n",
    "            # (f13, f14 (divide at end)), f15, f16, f17, f18\n",
    "            if row['source'] == tmp_src:\n",
    "                if(time_last_f == -1):\n",
    "                    time_last_f = row['time']\n",
    "                \n",
    "                t_diff_f = row['time']-time_last_f \n",
    "                \n",
    "                f[12] += t_diff_f\n",
    "                f_count+=1\n",
    "                \n",
    "                f[14] = max(f[14], t_diff_f)\n",
    "                if t_diff_f != 0.0:\n",
    "                    f[15] = min(f[15], t_diff_f)\n",
    "            else:\n",
    "                if(time_last_b == -1):\n",
    "                    time_last_b = row['time']\n",
    "                    \n",
    "                t_diff_b = row['time']-time_last_b \n",
    "                \n",
    "                f[13] += t_diff_b\n",
    "                b_count+=1\n",
    "                \n",
    "                f[16] = max(f[16], t_diff_b)\n",
    "                if t_diff_b != 0.0:\n",
    "                    f[17] = min(f[17], t_diff_b)\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "        # FILTERING for flows with bidirectional packets and time window > 1/4 of the pcap field capturing window i.e. 1/4 * (3600) secs\n",
    "        if( i<=2 or f_count<=1 or b_count<=1 or f[10] < 900):\n",
    "            # dont append these features\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # f1, f12\n",
    "        f[0] = f[0]/(i-2)     # len(groups.get_group(key))-1\n",
    "        f[11] = f[10]/(i-2)   # len(groups.get_group(key))-1\n",
    "        \n",
    "        #f13, f14 \n",
    "        if f_count > 0:\n",
    "            f[12] /= f_count \n",
    "        if b_count > 0:\n",
    "            f[13] /= b_count \n",
    "        \n",
    "        # 3601.0 marks unavailability of feature\n",
    "        # WILL NEVER OCCUR (as such cases already filtered)\n",
    "        for i in [8,9,12,13,14,15,16,17]:\n",
    "            if f[i]==0 or f[i] == 0.0:\n",
    "                f[i]=3601.0\n",
    "        \n",
    "\n",
    "        features.append(f)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features util\n",
    "def get_feats(dfs, file_c):\n",
    "    feats = []\n",
    "    for df in dfs:\n",
    "        fs = extr_feats_tr(df, file_c)\n",
    "        try:\n",
    "            if fs and len(fs)>0:\n",
    "                feats.extend(fs)\n",
    "        except Exception as e:\n",
    "            print(\"Caught exception while adding feats:\", e)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_csv(files, file_c):\n",
    "    ans = []\n",
    "   \n",
    "    for file in files:\n",
    "        print(\"PARSING \"+file)\n",
    "        dfs = parse_cap_tr(file)\n",
    "        if dfs:\n",
    "            print(\"EXTRACTING FEATURES...\")\n",
    "            feats = get_feats(dfs, file_c)\n",
    "            ans.extend(feats)\n",
    "    \n",
    "    print('EXTRACTED FEATURES!')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_csv_parallel(files, csv_name, file_c):\n",
    "    l = len(files)\n",
    "    f_list = [files[math.floor(i*(l/16)):math.floor((i+1)*l/16)] for i in range(16)]\n",
    "    \n",
    "    ans = []\n",
    "    \n",
    "    with open(csv_name, 'w') as f:\n",
    "        f.write('F1,F2,F3,F4,F5,F6,F7,F8,F9,F10,F11,F12,F13,F14,F15,F16,F17,F18,TMP_SRC_IP,TMP_DST_IP,SRC_P,DST_P,PROTO,IPMARK\\n')\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as e:\n",
    "        futures = [e.submit(build_csv, files, file_c) for files in f_list]\n",
    "        i=0\n",
    "        for f in futures: \n",
    "            if f.result() and len(f.result())>0:\n",
    "                ans.extend(f.result())\n",
    "            i+=1\n",
    "    \n",
    "    with open(csv_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(ans)\n",
    "        print(\"DONE\", end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def util(x):\n",
    "    if x == 'v':\n",
    "        build_csv_parallel(BOT_V, './csv_generated/bot_v.csv', x)\n",
    "    elif x == 'z':\n",
    "        build_csv_parallel(BOT_Z, './csv_generated/bot_z.csv', x)\n",
    "    elif x =='s':\n",
    "        build_csv_parallel(BOT_S, './csv_generated/bot_s.csv', x)\n",
    "    elif x =='1':\n",
    "        build_csv_parallel(BEN_1,'./csv_generated/ben_1.csv', x)\n",
    "    elif x =='2':\n",
    "        build_csv_parallel(BEN_2,'./csv_generated/ben_2.csv', x)\n",
    "    elif x =='t':\n",
    "        build_csv_parallel(BEN_T,'./csv_generated/ben_t.csv', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util('v')\n",
    "util('z')\n",
    "util('s')\n",
    "util('t')\n",
    "util('1')\n",
    "util('2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
